{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# ADSP Big Data and Cloud Computing Final Project\n## Part 1\n## Author: Jingkai Wang\n### Date: Nov 29, 2024"}, {"cell_type": "markdown", "metadata": {}, "source": "## Start Code"}, {"cell_type": "markdown", "metadata": {}, "source": "## Reading Git Final Project"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "import os\nimport subprocess\nimport datetime\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode, length, expr\nfrom pyspark.sql.functions import col, from_unixtime, to_date, regexp_extract\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "gcs_folder = 'gs://msca-bdp-data-open/final_project_git'"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Check data size in GCS"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Total directory size: 1.36 TiB     gs://msca-bdp-data-open/final_project_git\n\n"}], "source": "cmd = 'gsutil du -s -h ' + gcs_folder\n\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\nfor line in p.stdout.readlines():\n    print (f'Total directory size: {line}')\n    \nretval = p.wait() # Wait for the child process to terminate."}, {"cell_type": "markdown", "metadata": {}, "source": "### Read Git data from GCS"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Languages\nProgramming languages by repository as reported by GitHub's https://developer.github.com/v3/repos/#list-languages API"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:=============================>                             (2 + 2) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *languages*: 3,325,634\nCPU times: user 11.9 ms, sys: 401 \u00b5s, total: 12.3 ms\nWall time: 10.3 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_languages = spark.read.parquet(os.path.join(gcs_folder, 'languages'))\nprint(f'Records read from dataframe *languages*: {df_languages.count():,.0f}')\n"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- bytes: long (nullable = true)\n\n"}], "source": "df_languages.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Licenses\nOpen source license SPDX code for each repository as detected by https://developer.github.com/v3/licenses/"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *licenses*: 3,325,634\nCPU times: user 3.74 ms, sys: 1.13 ms, total: 4.88 ms\nWall time: 1 s\n"}], "source": "%%time   \n    \ndf_licenses = spark.read.parquet(os.path.join(gcs_folder, 'licenses'))\nprint(f'Records read from dataframe *licenses*: {df_licenses.count():,.0f}')"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- license: string (nullable = true)\n\n"}], "source": "df_licenses.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Commits\nUnique Git commits from open source repositories on GitHub, pre-grouped by repositories they appear in."}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 11:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *commits*: 265,419,190\nCPU times: user 101 ms, sys: 19.7 ms, total: 121 ms\nWall time: 32.6 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_commits = spark.read.parquet(os.path.join(gcs_folder, 'commits'))\nprint(f'Records read from dataframe *commits*: {df_commits.count():,.0f}')\n"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- commit: string (nullable = true)\n |-- tree: string (nullable = true)\n |-- parent: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- author: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- email: string (nullable = true)\n |    |-- time_sec: long (nullable = true)\n |    |-- tz_offset: long (nullable = true)\n |    |-- date: struct (nullable = true)\n |    |    |-- seconds: long (nullable = true)\n |    |    |-- nanos: long (nullable = true)\n |-- committer: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- email: string (nullable = true)\n |    |-- time_sec: long (nullable = true)\n |    |-- tz_offset: long (nullable = true)\n |    |-- date: struct (nullable = true)\n |    |    |-- seconds: long (nullable = true)\n |    |    |-- nanos: long (nullable = true)\n |-- subject: string (nullable = true)\n |-- message: string (nullable = true)\n |-- trailer: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- key: string (nullable = true)\n |    |    |-- value: string (nullable = true)\n |    |    |-- email: string (nullable = true)\n |-- difference: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- old_mode: long (nullable = true)\n |    |    |-- new_mode: long (nullable = true)\n |    |    |-- old_path: string (nullable = true)\n |    |    |-- new_path: string (nullable = true)\n |    |    |-- old_sha1: string (nullable = true)\n |    |    |-- new_sha1: string (nullable = true)\n |    |    |-- old_repo: string (nullable = true)\n |    |    |-- new_repo: string (nullable = true)\n |-- difference_truncated: boolean (nullable = true)\n |-- repo_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- encoding: string (nullable = true)\n\n"}], "source": "df_commits.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Contents\nUnique file contents of text files under 1 MiB on the HEAD branch.  \nCan be joined to `files` dataset using the id columns to identify the repository and file path."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 15:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *commits*: 281,191,977\nCPU times: user 69.4 ms, sys: 26.9 ms, total: 96.2 ms\nWall time: 25.2 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_contents = spark.read.parquet(os.path.join(gcs_folder, 'contents'))\nprint(f'Records read from dataframe *commits*: {df_contents.count():,.0f}')"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: string (nullable = true)\n |-- size: long (nullable = true)\n |-- content: string (nullable = true)\n |-- binary: boolean (nullable = true)\n |-- copies: long (nullable = true)\n\n"}], "source": "df_contents.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Files\nFile metadata for all files at HEAD.  \nJoin with `contents` dataset on id columns to search text."}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 17:===================================================>(1079 + 1) / 1080]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *files*: 2,309,424,945\nCPU times: user 14.6 ms, sys: 4.31 ms, total: 18.9 ms\nWall time: 6.45 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_files = spark.read.parquet(os.path.join(gcs_folder, 'files'))\nprint(f'Records read from dataframe *files*: {df_files.count():,.0f}')"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- ref: string (nullable = true)\n |-- path: string (nullable = true)\n |-- mode: long (nullable = true)\n |-- id: string (nullable = true)\n |-- symlink_target: string (nullable = true)\n\n"}], "source": "df_files.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Project code"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 1: Discard irrelevant or obviously erroneous data\nMost of the variable names should be self-explanatory, however data is deeply nested and will require detailed review in order to select the most appropriate data elements\n### Step 2: Complete thorough EDA to identify which variables you can use to complete your analysis\nAny poorly populated or duplicate variables should be discarded"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 1.8 ms, sys: 527 \u00b5s, total: 2.33 ms\nWall time: 21.4 ms\n"}], "source": "%%time\n# Set a sample to analyze data\nsample = 0.1\ndf_languages = df_languages.sample(withReplacement=False, fraction=sample)\ndf_licenses = df_licenses.sample(withReplacement=False, fraction=sample)\ndf_commits = df_commits.sample(withReplacement=False, fraction=sample)\ndf_contents = df_contents.sample(withReplacement=False, fraction=sample)\ndf_files = df_files.sample(withReplacement=False, fraction=sample)"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Records count 782324\nroot\n |-- repo_name: string (nullable = true)\n |-- language: string (nullable = true)\n |-- bytes: long (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>repo_name</th><th>language</th><th>bytes</th></tr>\n<tr><td>jnoller/talk-ment...</td><td>CSS</td><td>19904</td></tr>\n<tr><td>wael101/Dnn.Platform</td><td>PLpgSQL</td><td>53478</td></tr>\n<tr><td>eur00t/jewel-game</td><td>CSS</td><td>10081</td></tr>\n<tr><td>antonyflour/Raspu...</td><td>JavaScript</td><td>18360</td></tr>\n<tr><td>janhui/test_engine</td><td>Python</td><td>1368139</td></tr>\n</table>\n", "text/plain": "+--------------------+--------+-------+\n|           repo_name|language|  bytes|\n+--------------------+--------+-------+\n|NSAMR/uk.ac.nsamr...|     CSS|2781949|\n|kirtgoh/gcc-vcg-p...|   Shell| 290218|\n|      ChrisOHu/vimrc|  Erlang|  10020|\n|gramic/rules_closure|Starlark| 250591|\n|portefaix/docker-...|     HCL|    206|\n+--------------------+--------+-------+"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "# for languages\n# Drop duplicates first\ndf_languages = df_languages.dropDuplicates()\n\n# Explode the 'language' field to flatten the nested structure\ndf_languages_flattened = df_languages.withColumn('language', explode('language'))\n\n# Select the relevant fields from the flattened structure\ndf_languages_extracted = df_languages_flattened.select(\n    'repo_name',\n    col('language.name').alias('language'),\n    col('language.bytes').alias('bytes'))\n\n# Clean df_languages\ndf_languages_cleaned = df_languages_extracted.filter(\n    (col('repo_name').isNotNull()) &\n    (col('language').isNotNull()) &\n    (col('bytes') > 0) \n).dropDuplicates()\n\n# show result\nprint('Records count', df_languages_cleaned.count())\ndf_languages_cleaned.printSchema()\ndf_languages_cleaned.limit(5)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Records count 332254\nroot\n |-- repo_name: string (nullable = true)\n |-- license: string (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>repo_name</th><th>license</th></tr>\n<tr><td>bennie/perl-Text-...</td><td>artistic-2.0</td></tr>\n<tr><td>ocefpaf/weather_a...</td><td>artistic-2.0</td></tr>\n<tr><td>alexneri/umi-sono...</td><td>artistic-2.0</td></tr>\n<tr><td>squell/bb-scripts</td><td>artistic-2.0</td></tr>\n<tr><td>ehmicky/Koi</td><td>artistic-2.0</td></tr>\n</table>\n", "text/plain": "+--------------------+------------+\n|           repo_name|     license|\n+--------------------+------------+\n|bennie/perl-Text-...|artistic-2.0|\n|ocefpaf/weather_a...|artistic-2.0|\n|alexneri/umi-sono...|artistic-2.0|\n|   squell/bb-scripts|artistic-2.0|\n|         ehmicky/Koi|artistic-2.0|\n+--------------------+------------+"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "# for licenses\n# Drop duplicates first\ndf_licenses = df_licenses.dropDuplicates()\n\n# Clean df_licenses\ndf_licenses_cleaned = df_licenses.filter(\n    (col('repo_name').isNotNull()) &\n    (col('license').isNotNull())\n)\n\n# print result\nprint('Records count', df_licenses_cleaned.count())\ndf_licenses_cleaned.printSchema()\ndf_licenses_cleaned.limit(5)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- commit: string (nullable = true)\n |-- author_name: string (nullable = true)\n |-- author_email: string (nullable = true)\n |-- author_date: string (nullable = true)\n |-- committer_name: string (nullable = true)\n |-- committer_email: string (nullable = true)\n |-- committer_date: string (nullable = true)\n |-- subject: string (nullable = true)\n |-- message: string (nullable = true)\n |-- repo_name: string (nullable = true)\n\n"}], "source": "# for commits\n# Extract commit_date and timestamp\ndf_commits_cleaned = df_commits.select(\n    col('commit'),\n    col('author.name').alias('author_name'),\n    col('author.email').alias('author_email'),\n    from_unixtime(col('author.date.seconds'), 'yyyy-MM-dd HH:mm:ss').alias('author_date'),\n    col('committer.name').alias('committer_name'),\n    col('committer.email').alias('committer_email'),\n    from_unixtime(col('committer.date.seconds'), 'yyyy-MM-dd HH:mm:ss').alias('committer_date'),\n    col('subject'),\n    col('message'),\n    expr('repo_name[0]').alias('repo_name')\n)\n\n# drop the duplicate\ndf_commits_cleaned = df_commits_cleaned.dropDuplicates()\n\n# Clean df_commits\ndf_commits_cleaned = df_commits_cleaned.filter(\n    (col('commit').isNotNull()) &\n    (col('repo_name').isNotNull()) &\n    (col('author_name').isNotNull()) &\n    (col('message').isNotNull()) &\n    (col('author_date') < F.current_date()) &\n    (col('committer_date') < F.current_date()) &\n    (length(F.col('message')) > 5)  # Exclude trivial messages\n)\n\n# print result\n#print('Records count', df_commits_cleaned.count())\ndf_commits_cleaned.printSchema()\n#df_commits_cleaned.limit(5)\n"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: string (nullable = true)\n |-- size: long (nullable = true)\n |-- content: string (nullable = true)\n |-- binary: boolean (nullable = true)\n |-- copies: long (nullable = true)\n\n"}], "source": "# For contents\n# Clean df_contents\ndf_contents_cleaned = df_contents.filter(\n    (F.col('content').isNotNull()) &\n    (~F.col('binary')) &\n    (F.col('size') > 100) &  # Minimum size\n    (F.col('size') < 1048576)  # Maximum size: 1 MB\n).dropDuplicates()\n\n\n# print result\n#print('Records count', df_contents_cleaned.count())\ndf_contents_cleaned.printSchema()\n#df_contents_cleaned.limit(5)\n"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- ref: string (nullable = true)\n |-- path: string (nullable = true)\n |-- mode: long (nullable = true)\n |-- id: string (nullable = true)\n\n"}], "source": "# For file\n# drop the column that all are null\ndf_files_cleaned = df_files.drop('symlink_target')\n\n# Drop duplicates\ndf_files_cleaned = df_files_cleaned.dropDuplicates()\n\n# Clean df_files\ndf_files_cleaned = df_files_cleaned.filter(\n    (col('repo_name').isNotNull()) &       \n    (col('path').isNotNull()) &     \n    (col('id').isNotNull())\n)\n\n# print result\n#print('Records count', df_files_cleaned.count())\ndf_files_cleaned.printSchema()\n#df_files_cleaned.limit(5)\n"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Export the cleaned data\n# storage path in general\ncleaned_data = 'gs://msca-bdp-students-bucket/notebooks/jingkaiw/cleaned_data'\n\n# Save the languages data\npath_languages = os.path.join(cleaned_data, 'df_languages_cleaned')\ndf_languages_cleaned.write.mode('overwrite').parquet(path_languages) # ensure save multiple times\n\n# Save licenses data\npath_licenses = os.path.join(cleaned_data, 'df_licenses_cleaned')\ndf_licenses_cleaned.write.mode('overwrite').parquet(path_licenses) # ensure save multiple times\n\n# Save commites data\npath_commits = os.path.join(cleaned_data, 'df_commits_cleaned')\ndf_commits_cleaned.write.mode('overwrite').parquet(path_commits) # ensure save multiple times\n\n# Save contents data\npath_contents = os.path.join(cleaned_data, 'df_contents_cleaned')\ndf_contents_cleaned.write.mode('overwrite').parquet(path_contents) # ensure save multiple times\n\n# Save files data\npath_files = os.path.join(cleaned_data, 'df_files_cleaned')\ndf_files_cleaned.write.mode('overwrite').parquet(path_files) # ensure save multiple times\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": "'Sun, 01 December 2024 18:59:20'"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "import datetime\nimport pytz\n\ndatetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}